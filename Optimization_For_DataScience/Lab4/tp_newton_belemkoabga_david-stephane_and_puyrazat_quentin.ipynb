{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB: quasi-Newton methods\n",
    "\n",
    "Author : Alexandre Gramfort, Jair Montoya, Pierre Ablin\n",
    "\n",
    "The objective of this lab session is to implement:\n",
    "- Newton method\n",
    "- DFP\n",
    "- BFGS\n",
    "- l-BFGS\n",
    "\n",
    "And to investigate their behaviors.\n",
    "\n",
    "You will need to use **line search methods**.\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 25th of november at 23:59**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called **Rendu TP du 25 novembre 2018**. This is where you submit your jupyter notebook file. \n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp_newton_belemkoabga_david-stephane_and_puyrazat_quentin.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"David-Stephane\"\n",
    "ln1 = \"Belemkoabga\"\n",
    "fn2 = \"Quentin\"\n",
    "ln2 = \"Puyrazat\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp_newton\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Demo using Gradient descent\n",
    "\n",
    "First import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the necessary function from the optim_utils.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optim_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1c7467b322de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moptim_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_solver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optim_utils'"
     ]
    }
   ],
   "source": [
    "from optim_utils import test_solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll only need the `test_solver` function.\n",
    "\n",
    "This function expects a function as parameter.\n",
    "\n",
    "The signature of the function `optimizer` to pass should be the following:\n",
    "\n",
    "`optimizer(x0, f, f_grad, f_hessian)`\n",
    "\n",
    "First, an example with a gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x0, f, f_grad, f_hessian=None):\n",
    "\n",
    "    default_step = 0.01\n",
    "    c1 = 0.0001\n",
    "    c2 = 0.9\n",
    "    max_iter = 200\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = False\n",
    "\n",
    "    all_x_k, all_f_k = [], []\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    for k in range(1, max_iter + 1):\n",
    "\n",
    "        grad_x = f_grad(x)\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              -grad_x, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        x -= step * grad_x\n",
    "\n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, call the `test_solver` function with this solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_solver(gradient_descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It runs the algorithm on three functions:\n",
    "- A non convex Gaussian kernel ($f(x) = -\\exp(-x^2)$)\n",
    "- A badly conditioned quadratic function (but still strongly convex)\n",
    "- The Rosenbrock function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Implement Newton method\n",
    "\n",
    "Implement Newton's method. Beware that the Hessian SHOULD be regularized !\n",
    "\n",
    "**You are expected to comment** what you see. Play with the parameters. Do not describe the curves, rather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "from scipy.sparse.linalg import cg\n",
    "from scipy.optimize import line_search\n",
    "\n",
    "c1 = 0.00001\n",
    "c2 = 0.95\n",
    "max_iter = 100\n",
    "lambda_threshold = 0.0001 # regularization threshold\n",
    "\n",
    "def newton(x0, f, f_grad, f_hessian):\n",
    "    default_step = 0.01\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = False\n",
    "\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    for k in range(1, max_iter + 1):\n",
    "\n",
    "        grad_x = f_grad(x)\n",
    "        \n",
    "        # Compute the Hessian, regularize it and compute the search direction d\n",
    "        \n",
    "        # TODO \n",
    "        H =f_hessian(x)\n",
    "        \n",
    "        v, w = np.linalg.eigh(H)\n",
    "        v[v < lambda_threshold] = lambda_threshold\n",
    "        H = (v * w).dot(w.T)\n",
    "        all_f_k.append(H.copy())\n",
    "        all_x_k.append(x.copy())\n",
    "        # Compute the search direction\n",
    "        direction = - np.linalg.solve(H, grad_x)\n",
    "        #alpha = line_search(f, f_grad, x, direction, grad_x, max_iter, c1, c2)[0]\n",
    "        #x += alpha * direction\n",
    "        \n",
    "        \n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              direction, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        # Compute here the new value of x\n",
    "        x += step * direction\n",
    "\n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Lamdba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = 0.0001\n",
    "c2 = 0.95\n",
    "lambda_threshold = 0.0001\n",
    "test_solver(newton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_threshold = 0.001\n",
    "test_solver(newton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_threshold = 0.01\n",
    "test_solver(newton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with C1 and C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = 0.0001\n",
    "c2 = 0.75\n",
    "lambda_threshold = 0.0001\n",
    "test_solver(newton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = 0.0001\n",
    "c2 = 0.90\n",
    "lambda_threshold = 0.0001\n",
    "test_solver(newton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Variation of Lambda:**\n",
    "\n",
    "Case 1: In the case of a non convex function, the convergence is slow in the beginning as the quadratic approximation of the objetive function isn't so good, but we can observe suddenly that in one step the minimun is found. This is due to the fact that the quadratic approximation is very close to the true function, and so the Newton method converges in one step (as in the second case). However, this convergence is slower than the algorithm with the Gradient Descent algorithm, before that this step is reached. <br><br>\n",
    "\n",
    "Case 2: As expected, the newton method converges in 1 iteration in the case of a quadratic function. The value of the regularization parameter is very important. Here, if we take a bigger value of lambda thresheold, the convergence takes more than one iteration and will make some jumps to try to converge. By choosing a smallest regularization value, the Hessian eigenvalues are less modified due to less penalization. Hence, we preserve more accurate information contained in the Hessian. We can see that the biggest the lambda threshold, the more iterations it takes to the algorithm to converge.<br><br>\n",
    "\n",
    "Case 3: For the Rosenbrock function, the convergence rate is quadratic. However, we can see that the algorithm doesn't converge to the global minimum. Indeed, it doesn t reach a stability at the end.\n",
    "\n",
    "**Variations of C1 and C2:** \n",
    "\n",
    "As explained just before, we have to choose a slow value of lambda and we will keep lambda_threshold equal to 0.0001 to converge in 1 step in the second case.\n",
    "\n",
    "As seen in the class, c1 and c2 are variables that allow us to ensure that the objective function decreases over time, by finding and optimal step size satisfying strong Wolfe's rule. But some values of these variables are better suited for the functions we have than others.<br>\n",
    "\n",
    "First, we can see that playing with parameters c1 and c2 only influence convergence of the first function, which is non convex. Taking a bigger value for c1 seems to lead to better line search results in this context. Indeed, as seen in the class, is is important to have a C1 lower than 1/2 and a C2 better than 1/2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Implement DFP algorithm\n",
    "\n",
    "Now, implement the DFP algorithm using the formula for $B$ in the slides.\n",
    "\n",
    "**Comment on what you observe**. Focus on the explanation, not on describing the curves! \n",
    "\n",
    "Isn't there a contradiction on the quadratic functions with what we've seen in class? What is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfp(x0, f, f_grad, f_hessian):\n",
    "    default_step = 0.01\n",
    "    c1 = 0.0001\n",
    "    c2 = 0.95\n",
    "    max_iter = 200\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = False\n",
    "\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    B = np.eye(len(x))  # inverse Hessian approximation, start from Id\n",
    "    \n",
    "    grad_x = f_grad(x)\n",
    "    \n",
    "    for k in range(1, max_iter + 1):       \n",
    "        \n",
    "        # Compute the search direction\n",
    "        d = np.dot(B, -grad_x)\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              d, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "        \n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        # Compute the new value of x\n",
    "        s = step * d\n",
    "        x = x + s\n",
    "        y = new_grad - grad_x\n",
    "        \n",
    "        ################################################################\n",
    "        # Update the inverse Hessian approximation\n",
    "        B+= (np.outer(s,s.T)/np.dot(s.T,y)) - B@np.outer(y,y.T)@B/(y.T@B@y)\n",
    "        ################################################################\n",
    "        \n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "            \n",
    "        grad_x = new_grad\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_solver(dfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1: In the first case, the convergence is quadratic as expected for a Quasi-Newton's method. The convergence of the function is faster than in with the Gradient Descent and the Newton algorithm<br><br>\n",
    "\n",
    "Case 2: The second function is a quadratic function, so we should expect a quadratic convergence as we've seen in class. But here, the problem is that the function is very bad conditioned. As a consequence, we can see that the convergence is not as smooth as expected<br>\n",
    "\n",
    "Case 3: In the third case, we can notice that the convergence is slow compared to Newton's method and Gradient Descent method.\n",
    "The method stays a lot of time near the same points before making a great advance. The line search may be the cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Implement BFGS algorithm\n",
    "\n",
    "You should now implement BFGS, using the formula for $B_t$ seen in the slides.\n",
    "\n",
    "**Comment** on what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(x0, f, f_grad, f_hessian):\n",
    "    default_step = 0.01\n",
    "    c1 = 0.0001\n",
    "    c2 = 0.9\n",
    "    max_iter = 100\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = False\n",
    "\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    B = np.eye(len(x))  # Hessian approximation\n",
    "    \n",
    "    grad_x = f_grad(x)\n",
    "    \n",
    "    for k in range(1, max_iter + 1):       \n",
    "        \n",
    "        # Compute the search direction\n",
    "        d = -np.dot(B, grad_x)\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              d, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "                \n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        # Compute the new value of x\n",
    "        s = step * d\n",
    "        x += s\n",
    "        #print(x)\n",
    "        y = new_grad - grad_x\n",
    "        \n",
    "        ##################################################################\n",
    "        # Update the inverse Hessian approximation\n",
    "        p = len(x)\n",
    "        rho = 1. / y.dot(s)\n",
    "        B = (np.eye(p) - rho * np.outer(s, y)).dot(B).dot(np.eye(p) - rho * np.outer(y, s)) + rho * np.outer(s, s)         \n",
    "        ##################################################################\n",
    "        \n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "            \n",
    "        grad_x = new_grad\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_solver(bfgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence with BFGS method is faster than with DFP method in every cases. We can see a significant improvement in the cases two and three.\n",
    "\n",
    "Case 1 : We can see that it only takes 4 iterations to find the minimum, whereas it took 6 iterations using DFP algorithm. This is because, as we've seen in class, BFGS is less sensitive to line search errors than DFP, therefore it's more efficient (it takes better steps). <br><br>\n",
    "\n",
    "Case 2 : As opposed to DFP, the algorithm only computes the approximation of the Hessian and not its inverse. This overcomes the problem of numerical instability when computing the inverse, as explaines in the class. Again, this is due to the fact that BFGS is less sensitive to line search errors. <br><br>\n",
    "\n",
    "Case 3 : Same reasoning as in case 1. The convergence curve is smoother than with DFP because the steps taken are almost every time in the good direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Implement l-BFGS algorithm\n",
    "\n",
    "You should now implement the l-BFGS algorithm. First, code the two-loops recursion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_loops(grad_x, m, s_list, y_list, rho_list, B0,k):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    grad_x : ndarray, shape (p,)\n",
    "        gradient at the current point\n",
    "    \n",
    "    m : int\n",
    "        memory size\n",
    "    \n",
    "    s_list : list of length m\n",
    "        the past m values of s\n",
    "    \n",
    "    y_list : list of length m\n",
    "        the past m values of y\n",
    "\n",
    "    rho_list : list of length m\n",
    "        the past m values of rho\n",
    "        \n",
    "    B0 : ndarray, shape (p, p)\n",
    "        Initial inverse Hessian guess\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r :  ndarray, shape (p,)\n",
    "        the L-BFGS direction\n",
    "    '''\n",
    "    q = grad_x.copy()\n",
    "    alpha_list = []\n",
    "    # TODO : first loop\n",
    "    #m=len(s_list)\n",
    "    for i in range(m-1, -1, -1):\n",
    "        alpha_list.insert(0, rho_list[i] * s_list[i].dot(q))\n",
    "        q -= alpha_list[0] * y_list[i]\n",
    "    r = np.dot(B0, q)\n",
    "    \n",
    "    # TODO: second loop\n",
    "    for i in range(m):\n",
    "        beta = rho_list[i] * y_list[i].dot(r)\n",
    "        r += (alpha_list[i] - beta) * s_list[i]\n",
    "    return -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbfgs(x0, f, f_grad, f_hessian):\n",
    "    default_step = 0.01\n",
    "    c1 = 0.0001\n",
    "    c2 = 0.9\n",
    "    max_iter = 100\n",
    "    m = 2\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = False\n",
    "\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    B0 = np.eye(len(x))  # Hessian approximation\n",
    "    \n",
    "    grad_x = f_grad(x)\n",
    "    \n",
    "    y_list, s_list, rho_list = [], [], []\n",
    "    for k in range(1, max_iter + 1):       \n",
    "        \n",
    "        # Compute the search direction\n",
    "        d = two_loops(grad_x, m, s_list, y_list, rho_list, B0,k)\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              d, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "                \n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        # Compute the new value of x\n",
    "        s = step * d\n",
    "        x += s\n",
    "        y = new_grad - grad_x\n",
    "        rho = 1 / np.dot(y, s)\n",
    "        ##################################################################\n",
    "        # Update the memory\n",
    "        y_list.append(y.copy())\n",
    "        s_list.append(s.copy())\n",
    "        rho_list.append(rho)\n",
    "        if len(y_list) > m:\n",
    "            y_list.pop(0)\n",
    "            s_list.pop(0)\n",
    "            rho_list.pop(0)\n",
    "        ##################################################################\n",
    "        \n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "            \n",
    "        grad_x = new_grad\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_solver(lbfgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are similar to the BFGS results, the only difference we can observe is in the third case, where the convergence progress is less smooth at the end. We can think that it is because l-BFGS computes an approximation of the Hessian found in the corresponding step of BFGS. \n",
    "\n",
    "For the other cases, the problem is probably not big enough, we mean by that that the dimension of the problem is too low to observe a notable difference between l-BFGS and BFGS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison of models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Models | GD | Newton | DFP | BFGS | L-BFGS |\n",
    "| --- | --- | --- |  --- |  --- | --- | \n",
    "|Case 1, error/nb iterr/conv? | 10^-6/20/T | 10^-8/12/T | 10^-6/10/T | 10^-6/6/T | 10^-6/6/T |\n",
    "|Case 2, error/nb iterr/conv? | 10^0/200/F | 10^-0/1/T | 10^-5/6/T | 10^-5/4/T | 10^-5/4/T |\n",
    "|Case 3, error/nb iterr/conv? | 10^-1/200/F | 10^-9/24/T | 10^-7/90/T | 10^-8/40/T | 10^-8/45/T |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
