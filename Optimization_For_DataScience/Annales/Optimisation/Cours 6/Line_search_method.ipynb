{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent with line search\n",
    "\n",
    "Author : Alexandre Gramfort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim_utils import mk_quad, mk_gauss, rosenbrock,\\\n",
    "    rosenbrock_prime, rosenbrock_hessian, LoggingFunction,\\\n",
    "    CountingFunction, super_fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x0, f, f_prime, default_step=1.):\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    for k in range(1, 30):\n",
    "        all_x_k.append(x)\n",
    "        all_f_k.append(f(x))\n",
    "        grad_x = f_prime(x)\n",
    "        x = x - default_step * grad_x\n",
    "        if np.abs(all_f_k[-1]) < 1e-16:\n",
    "            break\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_line_search(x0, f, f_prime, default_step=1.,\n",
    "                                 c1=0.0001, c2=0.9):\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    for k in range(1, 30):\n",
    "        all_x_k.append(x)\n",
    "        all_f_k.append(f(x))\n",
    "        grad_x = f_prime(x)\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # Wolfe conditions\n",
    "        step = optimize.line_search(f, f_prime, x, -grad_x, grad_x, c1=c1, c2=c2)[0]\n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "    \n",
    "        x = x - step * grad_x\n",
    "        if np.abs(all_f_k[-1]) < 1e-16:\n",
    "            break\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(ax, all_x_k, all_f_k, all_x, x_min, x_max, y_min, y_max):\n",
    "    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n",
    "    x = x.T\n",
    "    y = y.T\n",
    "\n",
    "    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n",
    "    z = np.apply_along_axis(f, 0, X)\n",
    "    log_z = np.log(z + .01)\n",
    "    ax.imshow(log_z,\n",
    "            extent=[x_min, x_max, y_min, y_max],\n",
    "            cmap=plt.cm.gray_r, origin='lower',\n",
    "            vmax=log_z.min() + 1.5*log_z.ptp())\n",
    "    contours = ax.contour(log_z,\n",
    "                        levels=levels.get(f, None),\n",
    "                        extent=[x_min, x_max, y_min, y_max],\n",
    "                        cmap=plt.cm.gnuplot, origin='lower')\n",
    "    levels[f] = contours.levels\n",
    "    ax.clabel(contours, inline=1, fmt=super_fmt, fontsize=14)\n",
    "\n",
    "    ax.plot(all_x_k[:, 0], all_x_k[:, 1], 'b-', linewidth=2)\n",
    "    ax.plot(all_x_k[:, 0], all_x_k[:, 1], 'k+')\n",
    "\n",
    "    ax.plot(all_x[:, 0], all_x[:, 1], 'k.', markersize=4)\n",
    "\n",
    "    ax.plot([0], [0], 'rx', markersize=12)\n",
    "\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    plt.draw()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = -1, 2\n",
    "y_min, y_max = 2.25/3*x_min - .2, 2.25/3*x_max - .2\n",
    "x_min *= 1.2\n",
    "x_max *= 1.2\n",
    "y_min *= 1.2\n",
    "y_max *= 1.2\n",
    "\n",
    "levels = dict()\n",
    "\n",
    "default_step = 0.01\n",
    "c1 = 0.0001\n",
    "c2 = 0.9\n",
    "\n",
    "c1 = 0.01\n",
    "c2 = 0.05\n",
    "\n",
    "def optimizer(x0, f, f_prime):\n",
    "    return gradient_descent_line_search(x0, f, f_prime,\n",
    "                                        default_step, c1, c2)\n",
    "\n",
    "# def optimizer(x0, f, f_prime):\n",
    "#     return gradient_descent(x0, f, f_prime, default_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n",
    "\n",
    "for index, [ax, (f, f_prime, _)] in enumerate(zip(axes, (\n",
    "                                mk_quad(.7),\n",
    "                                mk_quad(.02),\n",
    "                                (rosenbrock, rosenbrock_prime, rosenbrock_hessian)))):\n",
    "\n",
    "    print(\"Running solver on case %d\" % (index + 1))\n",
    "\n",
    "    # Run optimization method logging all the function calls\n",
    "    logging_f = LoggingFunction(f)\n",
    "    x0 = np.array([1.6, 1.1])\n",
    "    all_x_k, all_f_k = optimizer(x0, logging_f, f_prime)\n",
    "\n",
    "    # Plot the convergence\n",
    "    all_x = np.array(logging_f.all_x)\n",
    "    plot_convergence(ax, all_x_k, all_f_k, all_x, x_min, x_max, y_min, y_max)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
