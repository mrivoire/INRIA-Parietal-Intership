{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coordinate_Descent_With_Gap_Safe_Rules_v1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnF7T4u681Yo",
        "colab_type": "text"
      },
      "source": [
        "<center> \n",
        "  <h1> Mind The Duality Gap : Safer Rules For The Lasso </h1> \n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YobSe-p3vCM7",
        "colab_type": "code",
        "outputId": "0f97e005-bd78-442e-e503-703423bd81f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!pip install python-intervals"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-intervals\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/51/b29570d4a820610be14d232aec77e6f0c66bca3d400f4903e98cc00012cb/python_intervals-1.10.0.post1-py2.py3-none-any.whl\n",
            "Installing collected packages: python-intervals\n",
            "Successfully installed python-intervals-1.10.0.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jgwRtVS95rB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import linalg\n",
        "from numpy.random import multivariate_normal\n",
        "from scipy.linalg.special_matrices import toeplitz \n",
        "from numpy.random import randn\n",
        "from intervals import Interval, inf\n",
        "from scipy.optimize import minimize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGyeLaJV9bzP",
        "colab_type": "text"
      },
      "source": [
        "One chooses Coordinate Descent as iterative solver. <br/>\n",
        "The idea of coordinate descent is to decompose a large optimisation problem into\n",
        "a sequence of one-dimensional optimisation problems. \n",
        "Coordinate descent methods have become unavoidable in machine learning because\n",
        "they are very efficient for key problems, namely Lasso, Logistic Regression and\n",
        "Support Vector Machines. <br/>\n",
        "Moreover, the decomposition into small subproblems means that only a small part \n",
        "of the data is processed at each iteration and this makes coordinate descent\n",
        "easily scalable to high dimensions. \n",
        "The idea of coordinate gradient descent is to perform one iteration of gradient\n",
        "in the 1-dimensional problem :\n",
        "$$\n",
        "min_{z \\in X_{i}} f(x_{k}^{(1)}, \\ldots, x_{k}^{(l-1)}, z, x_{k}^{(l+1)}, \\ldots, x_{k}^{(n)})\n",
        "$$\n",
        "instead of solving it completely. In general it reduces drastically the cost \n",
        "of each iteration while keeping the same convergence behaviour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvLJqk-B-Bcc",
        "colab_type": "text"
      },
      "source": [
        "## Data Simulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu7nS00P-Ack",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simu(coefs, n_samples=1000, corr=0.5, for_logreg=False):\n",
        "    n_features=len(coefs)\n",
        "    cov = toeplitz(corr ** np.array2string(0, n_features))\n",
        "\n",
        "    # Features Matrix\n",
        "    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
        "\n",
        "    # Target labels vector with noise\n",
        "    y = X.dot(coefs) + randn(n_samples)\n",
        "\n",
        "    if for_logreg:\n",
        "        y = np.sign(y)\n",
        "\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8xEb5zt-PjM",
        "colab_type": "text"
      },
      "source": [
        "## Coordinate Descent Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5QYv5157fCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cyclic_coordinate_descent(X, y, n_iter=10):\n",
        "    \"\"\"Solver : cyclic coordinate descent \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    X: numpy.ndarray, shape (n_samples, n_features)\n",
        "       features matrix\n",
        "\n",
        "    y: numpy.array, shape (n_samples, )\n",
        "       labels vector\n",
        "\n",
        "    n_iter: int, default = 10\n",
        "            number of iterations  \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    w: numpy.array, shape(n_features,)\n",
        "       weights vector\n",
        "\n",
        "    all_objs: numpy.array, shape(n_features)\n",
        "            residuals vector\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialisation of the parameters\n",
        "\n",
        "    n_samples, n_features = X.shape\n",
        "    all_objs = []\n",
        "\n",
        "    w = np.zeros(n_features)\n",
        "    residuals = y - X.dot(w)\n",
        "\n",
        "    # Computation of the lipschitz constants vector \n",
        "\n",
        "    lips_const = np.linalg.norm(X, axis=0)**2\n",
        "\n",
        "    # Iterations of the algorithm\n",
        "    for k in range(n_iter):\n",
        "\n",
        "        # One cyclicly updates the i^{th} coordinate corresponding to the rest \n",
        "        # in the Euclidean division by the number of features \n",
        "        # This allows to always selecting an index between 1 and n_features\n",
        "        i = k % n_features + 1\n",
        "\n",
        "        old_w_i = w[i].copy()\n",
        "        step = 1/lips_const[i]\n",
        "        grad = (X[:,i].T).dot(residuals)\n",
        "\n",
        "        # Update of the parameters\n",
        "        w[i] += step*grad \n",
        "\n",
        "        # Update of the residuals\n",
        "        residuals += np.dot(X[:,i], old_w_i - w[i])\n",
        "\n",
        "        if k % n_features == 0:\n",
        "            # If k % n_features == 0 then we have updated all the coordinates\n",
        "            # One computes the objective function \n",
        "            residuals += np.dot((residuals**2).sum()/2)\n",
        "\n",
        "    return w, np.array(all_objs)       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERmW5O_yORSk",
        "colab_type": "text"
      },
      "source": [
        "## Required Functions To The Coordinate Descent With Gap Safe Rules Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipzuV7dKdZ0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Equation 11\n",
        "def compute_theta_k(X, y, beta, lmbda):\n",
        "  \"\"\"Iterative computation of the dual optimal solution theta\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "\n",
        "  X: numpy.ndarray, shape = (n_samples, n_features)\n",
        "     features matrix\n",
        "\n",
        "  y: numpy.array, shape = (n_features, )\n",
        "     target labels vector\n",
        "\n",
        "  lmbda: numpy.array, shape = (n_iter, )\n",
        "         regularization parameters vector\n",
        "\n",
        "  beta_k: numpy.array, shape = (n_features, )\n",
        "          primal optimal parameters vector\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "\n",
        "  theta_k: float \n",
        "           dual optimal parameters vector\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialization of the parameters \n",
        "  rho_k = y - X @ beta_k\n",
        "  alpha_k = np.min(np.max((y @ rho_k)/(lmbda*np.linalg.norm(rho_k)**2), -1/np.linalg.nomr(X.T @ rho_k, ord='inf')), 1/np.linalg.norm(X.T @ rho_k, ord='inf'))\n",
        "  theta_k = alpha_k * rho_k\n",
        "\n",
        "  return theta_k\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtyctULMpAi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def radius(beta, theta, lmbda_t, lmbda_t_1, r_lmbda_t_1):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "\n",
        "  beta: numpy.array, shape = (n_features, )\n",
        "        primal optimal parameters vector\n",
        "\n",
        "  theta: numpy.array, shape = (n_features, )\n",
        "         dual optimal parameters vector\n",
        "\n",
        "  lmbda_t: float\n",
        "           regularization parameter at iteration t\n",
        "\n",
        "  lmbda_t_1: float\n",
        "             regularization parameter at iteration t-1\n",
        "  \n",
        "  r_lmbda_t_1: float\n",
        "               radius related to the regularization parameter lmbda_t_1\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  r_lmbda_t: float\n",
        "             radius related to the regularization parameter lmbda_t \n",
        "  \"\"\"\n",
        "\n",
        "  r_square_lmbda_t = (lmbda_t_1/lmbda_t)*r_lmbda_t_1**2 + (1 - lmbda_t/lmbda_t_1)*np.linalg.norm((X @ beta - y)/lmbda_t)**2 - (lmbda_t_1/lmbda_t - 1)*np.linalg.norm(theta)**2\n",
        "  r_lmbda_t = np.sqrt(r_square_lmbda_t)\n",
        "\n",
        "  return r_lmbda_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91PnNP1rsbw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Equation 18 : Gap Safe Sphere\n",
        "\n",
        "def gap_safe_sphere(theta_k, r_lmbda):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "\n",
        "  theta_k: numpy.array, shape = (n_features,)\n",
        "           dual optimal parameters vector\n",
        "  \n",
        "  r_lmbda: float\n",
        "           radius related to the regularization parameter lmbda\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "\n",
        "  C_k: interval\n",
        "       sphere of center theta_k and of radius r_lmbda\n",
        "  \"\"\"\n",
        "\n",
        "  inf_bound = theta_k - r_lmbda\n",
        "  sup_bound = theta_k + r_lmbda\n",
        "  C_k = interval[inf_bound, sup_bound]\n",
        "  \n",
        "  return C_k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keF9Mj6LyTTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def R_primal(lmbda, y, X, beta):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "\n",
        "  lmbda: float\n",
        "         regularization parameter\n",
        "  \n",
        "  y: numpy.array, shape=(n_samples, )\n",
        "\n",
        "  X: numpy.ndarray, shape=(n_samples, n_features)\n",
        "     features matrix \n",
        "\n",
        "  beta: numpy.array, shape=(n_features, )\n",
        "        primal optimal parameters\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "\n",
        "  R_hat_lmbda: float\n",
        "               primal radius of the dome\n",
        "  \"\"\"\n",
        "\n",
        "  R_hat_lmbda = (1/lmbda)*np.max(np.linalg.norm(y)**2 - np.linalg.norm(X @ beta - y)**2 - 2*lmbda*np.linalg.norm(beta, ord=1), 0)**(1/2)\n",
        "\n",
        "  return R_hat_lmbda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zci0OfYHzsNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def R_dual(y, theta, lmbda):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "\n",
        "  lmbda: float\n",
        "         regularization parameter\n",
        "  \n",
        "  y: numpy.array, shape=(n_samples, ) \n",
        "\n",
        "  theta: numpy.array, shape=(n_features, )\n",
        "        dual optimal parameters\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "\n",
        "  R_inv_hat_lmbda: float\n",
        "                   dual radius of the dome\n",
        "  \"\"\"\n",
        "\n",
        "  R_inv_hat_lmbda = np.linalg.norm(theta - y/lmbda)\n",
        "\n",
        "  return R_inv_hat_lmbda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUoc5wkKxWoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Equation 19 : Gap Safe Dome\n",
        "def gap_safe_dome(y, lmbda, theta_k, beta_k, R_hat_lmbda, R_inv_hat_lmbda):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ---------\n",
        "\n",
        "  y: np.array, shape = (n_samples, )\n",
        "\n",
        "  lmbda: float \n",
        "         regularization parameter\n",
        "  \n",
        "  theta_k: np.array, shape = (n_features, )\n",
        "           dual optimal parameters vector\n",
        "\n",
        "  beta_k: np.array, shape = (n_features, )\n",
        "          primal optimal parameters vector\n",
        "  \n",
        "  R_hat_lmbda: float\n",
        "               primal radius of the dome\n",
        "\n",
        "  R_inv_hat_lmbda: float\n",
        "                   dual radius of the dome\n",
        "  \n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "\n",
        "  C_k: interval\n",
        "       gape safe dome\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  c_k = ((y/lmbda) + theta_k)/2\n",
        "  r_k = R_inv_hat_lmbda/2\n",
        "  sphere_k = interval[c_k - r_k, c_k + r_k]\n",
        "\n",
        "  alpha_k = 2*(R_hat_lmbda/R_inv_hat_lmbda)**2 - 1\n",
        "  w_k = (theta_k - y/lmbda)/np.linalg.norm(theta_k - y/lmbda)\n",
        "\n",
        "  margin_k = - alpha_k*r_k*w_k\n",
        "\n",
        "  # c_k - alpah_k*r_k*w_k is the projection of the ball center c on the hyperplane\n",
        "  # c = ball center\n",
        "  # r = ball radius\n",
        "  # oriented hyperplane with unit normal vector w and parameter alpha \n",
        "  # such that c - alpha*r*w is the projection of c on the hyperplane\n",
        "\n",
        "  C_k = sphere_k.intersection(margin_k) # probl√®me d'intersection avec l'hyperplane\n",
        "\n",
        "  return C_k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQc8HFRW8ZHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigma_C(x, x_0):\n",
        "  \"\"\"Support Function\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "\n",
        "  x: np.array, shape = (n_samples, )\n",
        "     vector of samples for a given feature \n",
        "\n",
        "  x_0: np.array, shape = (n_samples, )\n",
        "       initial values \n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "\n",
        "  sigma: float\n",
        "         maximum of the scalar product <x, theta> w.r.t theta\n",
        "  \"\"\"\n",
        "\n",
        "  sigma = x @ theta \n",
        "  res = -minimize(-sigma, x_0, method=cd_with_gap_safe_rules, options={'xatol': 1e-8, 'disp': True})\n",
        "\n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRzJdP1e9lbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mu_C(x_j):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_j: np.array, shape = (n_samples, )\n",
        "       feature x_j \n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  mu: float \n",
        "      maximum between two sigma_C(x_j) and sigma_C(-x_j)\n",
        "  \"\"\"\n",
        "\n",
        "  mu = np.max(sigma_C(x_j), sigma_C(-x_j))\n",
        "\n",
        "  return mu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph4uQljp7d19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Equation 7 : Active Set\n",
        "def active_set_vs_zero_set(X):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  X: numpy.ndarray, shape = (n_samples, n_features)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  A_C: numpy.array, shape = (n_idx_active_features, )\n",
        "       active set : contains the indices of the relevant features\n",
        "  \n",
        "  Z_C: numpy.array, shape = (n_idx_zero_features, )\n",
        "       zero set : contains the indices of the irrelevant features\n",
        "  \"\"\"\n",
        "\n",
        "  A_C = []\n",
        "  Z_C = []\n",
        "  p = X.shape[1]\n",
        "  for j in range(p):\n",
        "    mu = mu_C(X[:,j])\n",
        "    if mu >= 1:\n",
        "      A_C.append(j)\n",
        "    else:\n",
        "      Z_C.append(j)\n",
        "\n",
        "  return A_C, Z_C"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2c2W7mbi-pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sign(x):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: float\n",
        "     \n",
        "  Returns\n",
        "  -------\n",
        "  s: sign int\n",
        "     (-1) if x < 0,, (+1) if x > 0, 0 if x = 0\n",
        "  \"\"\"\n",
        "\n",
        "  if x > 0:\n",
        "    s = 1\n",
        "  elif x < 0:\n",
        "    s = -1\n",
        "  else:\n",
        "    s = 0\n",
        "\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry19OgPy9qJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def soft_thresholding(u,x):\n",
        "  \"\"\"\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  u: float\n",
        "     threshold\n",
        "\n",
        "  x: float\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  ST: float\n",
        "      0 between -u and +u or slope of the straight line x - u otherwise \n",
        "      \n",
        "  \"\"\"\n",
        "\n",
        "  ST = sign(x)*np.max(np.abs(x) - u, 0)\n",
        "\n",
        "\n",
        "  return ST"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EceRmIc8nKcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Equation 1 : Primal Problem \n",
        "def primal_pb(X, y, beta, lmbda):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  X: numpy.ndarray, shape = (n_samples, n_features)\n",
        "     features matrix\n",
        "\n",
        "  y: numpy.array, shape = (n_samples, )\n",
        "     target labels vector\n",
        "\n",
        "  beta: numpy.array, shape = (n_features, )\n",
        "        initial vector of primal parameters\n",
        "\n",
        "  lmbda: float\n",
        "         regularization parameter\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "\n",
        "  beta_hat_lmbda: numpy.array shape = (n_features, )\n",
        "                  primal optimal parameters vector\n",
        "  \"\"\"\n",
        "\n",
        "  P_lmbda = (1/2)*np.linalg.norm(X @ beta - y, ord=2)**2 + lmbda*np.linalg.norm(beta, ord=1)\n",
        "  beta_hat_lmbda = minimize(P_lmbda, beta, method=cd_with_gap_safe_rules)\n",
        "\n",
        "  return beta_hat_lmbda\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa6drbIXq_6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dual_pb(y, theta, lmbda):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  y: numpy.array, shape = (n_features, )\n",
        "\n",
        "  theta: numpy.array, shape = (n_features, )\n",
        "         initial vector of dual parameters\n",
        "\n",
        "  lmbda: float\n",
        "         regularization parameter\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "\n",
        "  theta_hat_lmbda: numpy.array, shape = (n_features, )\n",
        "                   dual optimal parameters vector\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  D_lmbda = (1/2)*np.linalg.norm(y, ord=2)**2 - ((lmbda**2)/2)*np.linalg.norm(theta - y/lmbda, ord=2)**2\n",
        "  theta_hat_lmbda = -minimize(-D_lmbda, theta, method=cd_with_gap_safe_rules) # Probleme du feasible set\n",
        "  return theta_hat_lmbda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0CMBVOPmuO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def duality_gap(beta_hat_lmbda, theta_hat_lmbda):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "\n",
        "  beta_hat_lmbda: numpy.array shape = (n_features, )\n",
        "                  primal optimal parameters vector\n",
        "\n",
        "  theta_hat_lmbda: numpy.array, shape = (n_features, )\n",
        "                   dual optimal parameters vector\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  G_lmbda: float\n",
        "           duality gap between the primal optimal and the dual optimal\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  G_lmbda = beta_hat_lmbda - theta_hat_lmbda\n",
        "\n",
        "  return G_lmbda\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zna6ze9OFFC",
        "colab_type": "text"
      },
      "source": [
        "## Coordinate Descent With Gap Safe Rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZnmXrKZOD4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cd_with_gap_safe_rules(X, y, epsilon, K, f, lmbda, T, lmbda_max):\n",
        "  \"\"\"Coordinate descent with gap safe rules \n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  X: numpy.ndarray, shape = (n_samples, n_features)\n",
        "     features matrix\n",
        "\n",
        "  y: numpy.array, shape = (n_features, )\n",
        "     target labels vector\n",
        "\n",
        "  epsilon: float \n",
        "           accuracy\n",
        "\n",
        "  T: int\n",
        "     number of epochs\n",
        "\n",
        "  K: int\n",
        "     number of iterations\n",
        "\n",
        "  f: int\n",
        "     frequency\n",
        "\n",
        "  lmbda: numpy.array, shape = (T-1, )\n",
        "          regularization parameters vector\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "\n",
        "  beta_lmbda_t: numpy.array, shape = (n_features, )\n",
        "                primal optimal parameters vector \n",
        "                for the regularization parameter lmbda at iteration t\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialization \n",
        "  lmbda = []\n",
        "  lmbda[0] = lmbda_max\n",
        "  beta = dict()\n",
        "  beta[\"lmbda0\"] = 0\n",
        "\n",
        "  for t in range(T-1):\n",
        "    beta = beta[\"lambda\" & (t-1)]\n",
        "\n",
        "    for k in range(K):\n",
        "      if k % f == 1:\n",
        "\n",
        "        # Computation of theta \n",
        "        # Equation 11\n",
        "        # Equation 18\n",
        "        # Equation 19\n",
        "        if G_lmbda(beta, theta) <= epsilon:\n",
        "          beta_lmbda = beta\n",
        "          break\n",
        "\n",
        "        for j in A_C:\n",
        "          beta_j = soft_thresholding(lmbda_t/np.linalg.norm(X[:,j])**2, beta_j - ((X[:,j].T @ (X @ beta - y))/np.linalg.norm(X[:,j])**2))\n",
        "\n",
        "  return beta_lmbda_t"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}